{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.033831,
     "end_time": "2020-11-12T21:45:21.610254",
     "exception": false,
     "start_time": "2020-11-12T21:45:21.576423",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Riiid Answer Correctness Competition\n",
    "\n",
    "**A Number : A02230980** \n",
    "\n",
    "**Name : Tyler Lott**\n",
    "\n",
    "## Method\n",
    "For my submission I used the general form set out in google's paper [\"Attention is all you need.\"](https://arxiv.org/abs/1706.03762)\n",
    "Basically a transformer with multihead attention is used for nlp translation. In this notebook the model is adapted to \n",
    "take in the interaction sequences and predict the probability of a question being correct or incorrect. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-7690b203409c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mriiideducation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\school\\Fall 2020\\DataScience\\Project\\riiideducation\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mcompetition\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmake_env\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0m__all__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'make_env'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'riiideducation.competition'"
     ],
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'riiideducation.competition'",
     "output_type": "error"
    }
   ],
   "source": [
    "# Imports \n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import gc\n",
    "import riiideducation\n",
    "import pandas as pd\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T21:45:21.829590Z",
     "iopub.status.busy": "2020-11-12T21:45:21.828593Z",
     "iopub.status.idle": "2020-11-12T21:45:21.831281Z",
     "shell.execute_reply": "2020-11-12T21:45:21.831978Z"
    },
    "papermill": {
     "duration": 0.042871,
     "end_time": "2020-11-12T21:45:21.832136",
     "exception": false,
     "start_time": "2020-11-12T21:45:21.789265",
     "status": "completed"
    },
    "tags": [],
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Load the data\n",
    "define data types while loading in to reduce memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dtype = {\n",
    "    'answered_correctly': 'int8',\n",
    "    # 'row_id': 'int64',\n",
    "    'timestamp': 'int64',\n",
    "    'user_id': 'int32',\n",
    "    'content_id': 'int16',\n",
    "    # 'content_type_id': 'int8',\n",
    "    'task_container_id': 'int16',\n",
    "    # 'user_answer': 'int8',\n",
    "    'prior_question_elapsed_time': 'float32',\n",
    "    'prior_question_had_explanation': 'boolean'\n",
    "}\n",
    "\n",
    "dtype_questions = {\n",
    "    'question_id': 'int32',\n",
    "    # 'bundle_id': 'int32',\n",
    "    'correct_answer': 'int8',\n",
    "    'part': 'int8',\n",
    "    # 'tags': 'object',\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.032764,
     "end_time": "2020-11-12T21:45:21.897842",
     "exception": false,
     "start_time": "2020-11-12T21:45:21.865078",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Read the training data from the competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T21:45:22.049886Z",
     "iopub.status.busy": "2020-11-12T21:45:22.049070Z",
     "iopub.status.idle": "2020-11-12T21:45:23.158984Z",
     "shell.execute_reply": "2020-11-12T21:45:23.158136Z"
    },
    "papermill": {
     "duration": 1.1529,
     "end_time": "2020-11-12T21:45:23.159107",
     "exception": false,
     "start_time": "2020-11-12T21:45:22.006207",
     "status": "completed"
    },
    "tags": [],
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    # '/kaggle/input/riiid-test-answer-prediction/train.csv',\n",
    "    'data/train.csv',\n",
    "    usecols=dtype.keys(),\n",
    "    dtype=dtype,\n",
    "    # nrows=10.123*10**7\n",
    "    # nrows=10**7\n",
    ")\n",
    "# group by the user id and only keep the last 2000 interactions for each user\n",
    "df = df[df.answered_correctly!=-1]\n",
    "df = df.groupby('user_id').head(2000)\n",
    "\n",
    "questions = pd.read_csv(\n",
    "    # '/kaggle/input/riiid-test-answer-prediction/questions.csv', \n",
    "    'data/questions.csv',\n",
    "    dtype=dtype_questions,\n",
    "    usecols=dtype_questions.keys(),\n",
    "    index_col='question_id'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.032485,
     "end_time": "2020-11-12T21:45:23.226899",
     "exception": false,
     "start_time": "2020-11-12T21:45:23.194414",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Transform the data\n",
    "\n",
    "Some methods to transform the data to usable forms, we are sorting the timestamps into categories and normalizing the \n",
    "elapsed time. We are also adding 1 to each column with zero in it as that is used as a padding token later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T21:45:23.373646Z",
     "iopub.status.busy": "2020-11-12T21:45:23.372696Z",
     "iopub.status.idle": "2020-11-12T21:45:23.376375Z",
     "shell.execute_reply": "2020-11-12T21:45:23.375835Z"
    },
    "papermill": {
     "duration": 0.049446,
     "end_time": "2020-11-12T21:45:23.376493",
     "exception": false,
     "start_time": "2020-11-12T21:45:23.327047",
     "status": "completed"
    },
    "tags": [],
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def transform_questions(questions):\n",
    "  part_ids = questions.part.max()+1\n",
    "  return questions, part_ids\n",
    "\n",
    "\n",
    "def transform_df(df, questions):\n",
    "    df['prior_question_elapsed_time'] = df['prior_question_elapsed_time'].fillna(0).astype(np.float32)/300000\n",
    "    # df['timestamp'] = df.timestamp.diff().fillna(0)\n",
    "    df['timestamp'] = df['timestamp'].fillna(0).astype(np.float32)/87425000000\n",
    "    bins = [0, .1, .2, .3, .4, .5, .6, .7, .8, .9, 1, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9 ,2]\n",
    "    labels = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
    "    df['timestamp'] =  pd.cut(df['timestamp'], bins=bins, labels=labels).fillna(1)\n",
    "    df['prior_question_had_explanation'] = df['prior_question_had_explanation'].fillna(False).astype(int)\n",
    "    content_ids = questions.index.max()+2\n",
    "    df = df.join(questions, on='content_id')\n",
    "    df['content_id'] += 1\n",
    "    df['task_container_id'] += 1\n",
    "    df['user_ids'] = df['user_id'].astype('category').cat.codes\n",
    "    task_container_ids = 10001\n",
    "    return df, content_ids, task_container_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T21:45:23.450255Z",
     "iopub.status.busy": "2020-11-12T21:45:23.449199Z",
     "iopub.status.idle": "2020-11-12T21:45:23.536748Z",
     "shell.execute_reply": "2020-11-12T21:45:23.536118Z"
    },
    "papermill": {
     "duration": 0.126994,
     "end_time": "2020-11-12T21:45:23.536881",
     "exception": false,
     "start_time": "2020-11-12T21:45:23.409887",
     "status": "completed"
    },
    "tags": [],
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "questions, part_ids = transform_questions(questions)\n",
    "df, content_ids, task_container_ids = transform_df(df, questions)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.034397,
     "end_time": "2020-11-12T21:45:23.612051",
     "exception": false,
     "start_time": "2020-11-12T21:45:23.577654",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Group rows by user id and putting them in a hashtable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T21:45:23.760765Z",
     "iopub.status.busy": "2020-11-12T21:45:23.759732Z",
     "iopub.status.idle": "2020-11-12T21:45:28.087758Z",
     "shell.execute_reply": "2020-11-12T21:45:28.086392Z"
    },
    "papermill": {
     "duration": 4.367904,
     "end_time": "2020-11-12T21:45:28.087920",
     "exception": false,
     "start_time": "2020-11-12T21:45:23.720016",
     "status": "completed"
    },
    "tags": [],
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "df = {uid: u.drop(columns='user_id') for uid, u in df.groupby('user_id')}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.033989,
     "end_time": "2020-11-12T21:45:28.156088",
     "exception": false,
     "start_time": "2020-11-12T21:45:28.122099",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T21:45:34.599895Z",
     "iopub.status.busy": "2020-11-12T21:45:34.592405Z",
     "iopub.status.idle": "2020-11-12T21:45:34.602825Z",
     "shell.execute_reply": "2020-11-12T21:45:34.602226Z"
    },
    "papermill": {
     "duration": 0.069486,
     "end_time": "2020-11-12T21:45:34.602956",
     "exception": false,
     "start_time": "2020-11-12T21:45:34.533470",
     "status": "completed"
    },
    "tags": [],
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# just some stuff I ctrl C ctrl V from StackOverflow (with little changes)\n",
    "# [1,2,3,4] --- w = 2 --[[1,2], [2,3], [3,4]] but 2D to 3D\n",
    "def rolling_window(a, w):\n",
    "    s0, s1 = a.strides\n",
    "    m, n = a.shape\n",
    "    return np.lib.stride_tricks.as_strided(\n",
    "        a, \n",
    "        shape=(m-w+1, w, n), \n",
    "        strides=(s0, s0, s1)\n",
    "    )\n",
    "\n",
    "def make_time_series(x, windows_size):\n",
    "  x = np.pad(x, [[ windows_size-1, 0], [0, 0]], constant_values=0)\n",
    "  x = rolling_window(x, windows_size)\n",
    "  return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def add_features_to_user(user):\n",
    "    # We add one to the column in order to have zeros as padding values\n",
    "    # Start Of Sequence token will be 3. \n",
    "    user['answered_correctly'] = user['answered_correctly'].shift(fill_value=2)+1\n",
    "    return user"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Extending the keras Sequence class to create our dataset of users"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class RiidSequence(tf.keras.utils.Sequence):\n",
    "\n",
    "  def __init__(self, users, windows_size, start=0, end=None):\n",
    "    self.users = users \n",
    "    self.windows_size = windows_size\n",
    "    # to convert indices to our keys\n",
    "    self.mapper = dict(zip(range(len(users)), users.keys()))\n",
    "    # start and end to easy generate training and validation\n",
    "    self.start = start\n",
    "    self.end = end if end else len(users)\n",
    "    # To know where the answered_correctly_column is\n",
    "    self.answered_correctly_index = list(self.user_example().columns).index('answered_correctly')\n",
    "        \n",
    "  def __len__(self):\n",
    "    return self.end-self.start\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    uid = self.mapper[idx+self.start]\n",
    "    user = self.users[uid].copy()\n",
    "    y = user['answered_correctly'].to_numpy().copy()\n",
    "    x = add_features_to_user(user)\n",
    "    return make_time_series(x, self.windows_size), y\n",
    "\n",
    "  def user_example(self):\n",
    "    # returns a single user to check if it works\n",
    "    uid = self.mapper[self.start]\n",
    "    return add_features_to_user(self.users[uid].copy())\n",
    "\n",
    "  # INFERENCE PART    \n",
    "  def get_user_for_inference(self, user_row):\n",
    "    # Picks a new user row and concats it to previous interactions if it was already stored.\n",
    "    \n",
    "    # If the sequence if shorter than the window size, then we pad it.\n",
    "\n",
    "    uid = user_row[self.answered_correctly_index]\n",
    "    user_row[self.answered_correctly_index] = 2 # SOS token\n",
    "    user_row = user_row[np.newaxis, ...]\n",
    "    if uid in self.users:\n",
    "      x = np.concatenate([self.users[uid], user_row])\n",
    "      # same as in training, we need to add one!!!\n",
    "      x[:, self.answered_correctly_index] = np.roll(x[:, self.answered_correctly_index], 1) + 1\n",
    "    else:\n",
    "      x = user_row\n",
    "     \n",
    "    if x.shape[0] < self.windows_size:\n",
    "      return np.pad(x, [[self.windows_size-x.shape[0], 0], [0, 0]])\n",
    "    elif x.shape[0] > self.windows_size:\n",
    "      return x[-self.windows_size:]\n",
    "    else:\n",
    "      return x\n",
    "\n",
    "  def update_user(self, uid, user):\n",
    "    # updates the user information by concatenating the new info to the end of the user sequence\n",
    "    if uid in self.users:\n",
    "      self.users[uid] = \\\n",
    "        np.concatenate([self.users[uid], user])[-self.windows_size:]\n",
    "    else:\n",
    "      self.users[uid] = user"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T21:45:34.684998Z",
     "iopub.status.busy": "2020-11-12T21:45:34.684026Z",
     "iopub.status.idle": "2020-11-12T21:45:34.695344Z",
     "shell.execute_reply": "2020-11-12T21:45:34.694726Z"
    },
    "papermill": {
     "duration": 0.058205,
     "end_time": "2020-11-12T21:45:34.695488",
     "exception": false,
     "start_time": "2020-11-12T21:45:34.637283",
     "status": "completed"
    },
    "tags": [],
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "test out the sequence generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "RiidSequence(df, 64).user_example().head(20)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T21:45:34.780211Z",
     "iopub.status.busy": "2020-11-12T21:45:34.779323Z",
     "iopub.status.idle": "2020-11-12T21:45:34.782888Z",
     "shell.execute_reply": "2020-11-12T21:45:34.783479Z"
    },
    "papermill": {
     "duration": 0.049266,
     "end_time": "2020-11-12T21:45:34.783646",
     "exception": false,
     "start_time": "2020-11-12T21:45:34.734380",
     "status": "completed"
    },
    "tags": [],
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "x, y = RiidSequence(df, 100)[0]\n",
    "# print(x)\n",
    "x.shape, y.shape  # batch size, window size, features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.035763,
     "end_time": "2020-11-12T21:45:34.926114",
     "exception": false,
     "start_time": "2020-11-12T21:45:34.890351",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Transformer Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T21:45:35.135490Z",
     "iopub.status.busy": "2020-11-12T21:45:35.133867Z",
     "iopub.status.idle": "2020-11-12T21:45:35.191427Z",
     "shell.execute_reply": "2020-11-12T21:45:35.190765Z"
    },
    "papermill": {
     "duration": 0.119531,
     "end_time": "2020-11-12T21:45:35.191588",
     "exception": false,
     "start_time": "2020-11-12T21:45:35.072057",
     "status": "completed"
    },
    "tags": [],
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# POSITION ENCODING\n",
    "\n",
    "def get_angles(pos, i, d_model):\n",
    "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "  return pos * angle_rates\n",
    "\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "  pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "  return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Masking\n",
    "\n",
    "I'm unsure if a look ahead mask is needed in our case since we are only predicting one output, not a sequence, but I \n",
    "used it anyways and it seemed to work"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(size):\n",
    "  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "  return mask  # (seq_len, seq_len)\n",
    "\n",
    "def create_padding_mask(seqs):\n",
    "  # We mask only those vectors of the sequence in which we have all zeroes \n",
    "  # (this is more scalable for some situations).\n",
    "  mask = tf.cast(tf.reduce_all(tf.math.equal(seqs, 0), axis=-1), tf.float32)\n",
    "  return mask[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Things for the multihead attention"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# basic attention\n",
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "  matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
    "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "  if mask is not None:\n",
    "    scaled_attention_logits += (mask * -1e9)  \n",
    "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "  output = tf.matmul(attention_weights, v)\n",
    "  return output, attention_weights\n",
    "\n",
    "    \n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads):\n",
    "    super(MultiHeadAttention, self).__init__()\n",
    "    self.num_heads = num_heads\n",
    "    self.d_model = d_model\n",
    "    \n",
    "    assert d_model % self.num_heads == 0\n",
    "    \n",
    "    self.depth = d_model // self.num_heads\n",
    "    \n",
    "    self.wq = tf.keras.layers.Dense(d_model)\n",
    "    self.wk = tf.keras.layers.Dense(d_model)\n",
    "    self.wv = tf.keras.layers.Dense(d_model)\n",
    "    \n",
    "    self.dense = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "  # method to split the layers into different heads\n",
    "  def split_heads(self, x, batch_size):\n",
    "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "  def call(self, v, k, q, mask):\n",
    "    batch_size = tf.shape(q)[0]\n",
    "    # feed through dense layer \n",
    "    q = self.wq(q)\n",
    "    k = self.wk(k)\n",
    "    v = self.wv(v)\n",
    "\n",
    "    # split out into the number of heads passed through\n",
    "    q = self.split_heads(q, batch_size)\n",
    "    k = self.split_heads(k, batch_size)\n",
    "    v = self.split_heads(v, batch_size)\n",
    "\n",
    "    scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "        q, k, v, mask)\n",
    "\n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "\n",
    "    # recombine all of the heads of attention\n",
    "    concat_attention = tf.reshape(scaled_attention, \n",
    "                                  (batch_size, -1, self.d_model))\n",
    "\n",
    "    output = self.dense(concat_attention)\n",
    "\n",
    "    return output, attention_weights\n",
    "\n",
    "# simple feed forward at the end of the attention layer\n",
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "  return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),\n",
    "      tf.keras.layers.Dense(d_model)\n",
    "  ])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# the encoder layer that the question relavant stuff goes through\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "    super(EncoderLayer, self).__init__()\n",
    "\n",
    "    self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "  def call(self, x, training, mask):\n",
    "\n",
    "    # passes x as all inputs to the multihead attention then normalizes\n",
    "    attn_output, _ = self.mha(x, x, x, mask) \n",
    "    attn_output = self.dropout1(attn_output, training=training)\n",
    "    out1 = self.layernorm1(x + attn_output) \n",
    "\n",
    "    # passes the outputs through a feed forward network then normalizes\n",
    "    ffn_output = self.ffn(out1)\n",
    "    ffn_output = self.dropout2(ffn_output, training=training)\n",
    "    out2 = self.layernorm2(out1 + ffn_output) \n",
    "\n",
    "    return out2\n",
    "  "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# decoder layer that the encoder output and the question relevant inputs will go through\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "    super(DecoderLayer, self).__init__()\n",
    "\n",
    "    self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "    self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    " \n",
    "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    \n",
    "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    \n",
    "  def call(self, x, enc_output, training, mask, look_ahead_mask=None):\n",
    "    # encoder layer for the question relevant stuff\n",
    "    attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask) \n",
    "    attn1 = self.dropout1(attn1, training=training)\n",
    "    out1 = self.layernorm1(attn1 + x)\n",
    "    \n",
    "    # encoder out gets put through the multiheaded attention layer as the q and k\n",
    "    attn2, attn_weights_block2 = self.mha2(\n",
    "        enc_output, enc_output, out1, mask)\n",
    "    attn2 = self.dropout2(attn2, training=training)\n",
    "    out2 = self.layernorm2(attn2 + out1)\n",
    "    \n",
    "    # output of the combined attention layer is fed through a feed forward network\n",
    "    ffn_output = self.ffn(out2) \n",
    "    ffn_output = self.dropout3(ffn_output, training=training)\n",
    "    out3 = self.layernorm3(ffn_output + out2)\n",
    "    \n",
    "    return out3, attn_weights_block1, attn_weights_block2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_series_model(n_features, content_ids, task_container_ids, part_ids, windows_size=100, d_model=32, num_heads=4, \n",
    "                     n_encoder_layers = 2):\n",
    "    # Input\n",
    "    inputs = tf.keras.Input(shape=(windows_size, n_features), name='inputs')\n",
    "    mask = create_padding_mask(inputs)\n",
    "    look_mask = create_look_ahead_mask(tf.shape(inputs)[1])\n",
    "    pos_enc = positional_encoding(windows_size, d_model)    \n",
    "    \n",
    "    # Divide branches   \n",
    "    timestamp = inputs[..., 0]\n",
    "    content_id = inputs[..., 1]\n",
    "    task_container_id = inputs[..., 2]\n",
    "    elapsed_time = inputs[..., 4]\n",
    "    answered_correctly = inputs[..., 3]\n",
    "    prior_exp = inputs[..., 5]\n",
    "    correct_answer = inputs[..., 6]\n",
    "    part = inputs[..., 7]\n",
    "    \n",
    "    # Create embeddings\n",
    "    \n",
    "    # first chunk, contains : content id, container id, part id, position\n",
    "    content_embeddings = tf.keras.layers.Embedding(content_ids, d_model)(content_id)\n",
    "    task_embeddings = tf.keras.layers.Embedding(task_container_ids, d_model)(task_container_id)\n",
    "    part_embeddings = tf.keras.layers.Embedding(part_ids, d_model)(part)\n",
    "    answer_embeddings = tf.keras.layers.Embedding(4, d_model)(correct_answer)\n",
    "    prior_exp_embeddings = tf.keras.layers.Embedding(2, d_model)(prior_exp)\n",
    "    \n",
    "    # third chunk, contains : answered correctly, elapsed time, time between, position\n",
    "    answered_correctly_embeddings = tf.keras.layers.Embedding(4, d_model)(answered_correctly)\n",
    "    elapsed_time_embeddings = tf.keras.layers.Dense(d_model, use_bias=False)(elapsed_time) # use a dense layer to embed this because its continuous\n",
    "    timestamp_embeddings = tf.keras.layers.Embedding(20, d_model)(timestamp)\n",
    "    \n",
    "    # Add embeddings\n",
    "    x = tf.keras.layers.Add()([\n",
    "        pos_enc,\n",
    "        content_embeddings,\n",
    "        task_embeddings,\n",
    "        part_embeddings,\n",
    "        prior_exp_embeddings,\n",
    "        answer_embeddings])\n",
    "    print(x.shape)\n",
    "    y = tf.keras.layers.Add()([\n",
    "        pos_enc,\n",
    "        answered_correctly_embeddings,\n",
    "        elapsed_time_embeddings,\n",
    "        timestamp_embeddings])\n",
    "\n",
    "    # encoder layers\n",
    "    for _ in range(n_encoder_layers):\n",
    "        x = EncoderLayer(d_model=d_model, num_heads=num_heads, dff=d_model*4, rate=0.1)(x, mask=mask)\n",
    "\n",
    "    # decoder layers\n",
    "    for _ in range(n_encoder_layers):\n",
    "        k, _, _ = DecoderLayer(d_model=d_model, num_heads=num_heads, dff=d_model*4, rate=0.1)(y, enc_output=x, mask=mask, look_ahead_mask=None)\n",
    "    \n",
    "    k = tf.keras.layers.GlobalAveragePooling1D()(k)\n",
    "    k = tf.keras.layers.Dropout(0.1)(k)\n",
    "    \n",
    "    # single output for our prediction\n",
    "    output = tf.keras.layers.Dense(1, activation='sigmoid', name='output')(k)\n",
    "    return tf.keras.Model(inputs, output, name='Transformer')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.050865,
     "end_time": "2020-11-12T21:45:35.538465",
     "exception": false,
     "start_time": "2020-11-12T21:45:35.487600",
     "status": "completed"
    },
    "tags": [],
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Set Hyperparameters and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_idx = int(len(df)*0.8)\n",
    "windows_size = 100\n",
    "epochs = 100\n",
    "patience = 2\n",
    "d_model = 64\n",
    "num_heads = 8\n",
    "n_encoder_layers = 3"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create a custom learning rate schedule to adjust the learning rate as the model converges"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super(CustomSchedule, self).__init__()\n",
    "    \n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "    \n",
    "  def __call__(self, step):\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps ** -1.5)\n",
    "    \n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.99, epsilon=1e-9)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### plot of the learning rate"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_learning_rate_schedule = CustomSchedule(d_model)\n",
    "\n",
    "plt.plot(temp_learning_rate_schedule(tf.range(40000, dtype=tf.float32)))\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.xlabel(\"Train Step\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Callback to save the best weights"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "SAVEPATH = f'checkpoints/Transformer_ep-{epochs}_{int(time.time())}.h5'\n",
    "weights_callback = tf.keras.callbacks.ModelCheckpoint(filepath=SAVEPATH, \n",
    "                                                      save_weights_only=True, \n",
    "                                                      monitor='val_AUC', \n",
    "                                                      mode='max', save_best_only=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### create train and validation sets and set the model parameters"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "s_train = RiidSequence(df, windows_size, start=0, end=train_idx)\n",
    "s_val = RiidSequence(df, windows_size, start=train_idx)\n",
    "\n",
    "n_features = s_train[0][0].shape[-1]\n",
    "print(n_features)\n",
    "print(content_ids)\n",
    "print(task_container_ids)\n",
    "print(part_ids)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create model and compile"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = get_series_model(\n",
    "        n_features,\n",
    "        content_ids,\n",
    "        task_container_ids,\n",
    "        part_ids,\n",
    "        windows_size=windows_size,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        n_encoder_layers=n_encoder_layers)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer, loss='binary_crossentropy',  metrics=[tf.keras.metrics.AUC(name='AUC'), tf.keras.metrics.BinaryAccuracy(name='acc')])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.summary()\n",
    "# load old weights to train from\n",
    "model.load_weights('checkpoints/Transformer_ep-100_1606742208.h5')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T21:45:36.334600Z",
     "iopub.status.busy": "2020-11-12T21:45:36.333360Z",
     "iopub.status.idle": "2020-11-12T21:45:40.905409Z",
     "shell.execute_reply": "2020-11-12T21:45:40.906748Z"
    },
    "papermill": {
     "duration": 4.638696,
     "end_time": "2020-11-12T21:45:40.906948",
     "exception": false,
     "start_time": "2020-11-12T21:45:36.268252",
     "status": "completed"
    },
    "tags": [],
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    s_train, validation_data=s_val, epochs=epochs, shuffle=True, callbacks=(weights_callback), verbose=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 2.879111,
     "end_time": "2020-11-12T21:55:11.381922",
     "exception": false,
     "start_time": "2020-11-12T21:55:08.502811",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Make prediction and submitt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 2.906814,
     "end_time": "2020-11-12T21:55:17.272501",
     "exception": false,
     "start_time": "2020-11-12T21:55:14.365687",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The API for submitting is funky so a lot of this is heavily influenced by other notebooks from the competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T21:55:30.821564Z",
     "iopub.status.busy": "2020-11-12T21:55:30.820491Z",
     "iopub.status.idle": "2020-11-12T21:55:30.827927Z",
     "shell.execute_reply": "2020-11-12T21:55:30.828482Z"
    },
    "papermill": {
     "duration": 3.834712,
     "end_time": "2020-11-12T21:55:30.828639",
     "exception": false,
     "start_time": "2020-11-12T21:55:26.993927",
     "status": "completed"
    },
    "tags": [],
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# delte stuff we don't need\n",
    "del s_val\n",
    "del df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T21:55:37.230971Z",
     "iopub.status.busy": "2020-11-12T21:55:37.229294Z",
     "iopub.status.idle": "2020-11-12T21:57:32.007043Z",
     "shell.execute_reply": "2020-11-12T21:57:32.006091Z"
    },
    "papermill": {
     "duration": 118.123295,
     "end_time": "2020-11-12T21:57:32.007203",
     "exception": false,
     "start_time": "2020-11-12T21:55:33.883908",
     "status": "completed"
    },
    "tags": [],
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Read in our data\n",
    "df = pd.read_csv(\n",
    "    # '../input/riiid-test-answer-prediction/train.csv',\n",
    "    'data/train.csv',\n",
    "    usecols=dtype.keys(),\n",
    "    dtype=dtype,\n",
    "    # nrows=10**6\n",
    ")\n",
    "df = df[df.answered_correctly!=-1]\n",
    "df = df.groupby('user_id').tail(windows_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T21:57:37.917276Z",
     "iopub.status.busy": "2020-11-12T21:57:37.915813Z",
     "iopub.status.idle": "2020-11-12T22:04:15.377349Z",
     "shell.execute_reply": "2020-11-12T22:04:15.376408Z"
    },
    "papermill": {
     "duration": 400.401828,
     "end_time": "2020-11-12T22:04:15.377496",
     "exception": false,
     "start_time": "2020-11-12T21:57:34.975668",
     "status": "completed"
    },
    "tags": [],
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Transform our data\n",
    "df, _, _ = transform_df(df, questions)\n",
    "df = {uid: u.drop(columns='user_id') for uid, u in df.groupby('user_id')}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T22:04:29.597267Z",
     "iopub.status.busy": "2020-11-12T22:04:29.596202Z",
     "iopub.status.idle": "2020-11-12T22:04:29.599333Z",
     "shell.execute_reply": "2020-11-12T22:04:29.598762Z"
    },
    "papermill": {
     "duration": 3.305175,
     "end_time": "2020-11-12T22:04:29.599454",
     "exception": false,
     "start_time": "2020-11-12T22:04:26.294279",
     "status": "completed"
    },
    "tags": [],
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# create api environment\n",
    "env = riiideducation.make_env()\n",
    "iter_test = env.iter_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T22:04:35.420735Z",
     "iopub.status.busy": "2020-11-12T22:04:35.419513Z",
     "iopub.status.idle": "2020-11-12T22:04:35.424263Z",
     "shell.execute_reply": "2020-11-12T22:04:35.424777Z"
    },
    "papermill": {
     "duration": 2.898988,
     "end_time": "2020-11-12T22:04:35.424948",
     "exception": false,
     "start_time": "2020-11-12T22:04:32.525960",
     "status": "completed"
    },
    "tags": [],
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "columns = list(RiidSequence(df, 64).user_example().columns)\n",
    "columns[columns.index('answered_correctly')] = 'user_id'\n",
    "columns = [c for c in columns if c not in questions.columns] + ['row_id']\n",
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T22:04:41.540808Z",
     "iopub.status.busy": "2020-11-12T22:04:41.532127Z",
     "iopub.status.idle": "2020-11-12T22:04:43.579156Z",
     "shell.execute_reply": "2020-11-12T22:04:43.577357Z"
    },
    "papermill": {
     "duration": 5.215103,
     "end_time": "2020-11-12T22:04:43.579320",
     "exception": false,
     "start_time": "2020-11-12T22:04:38.364217",
     "status": "completed"
    },
    "tags": [],
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for test, sample_prediction in iter_test:\n",
    "    \n",
    "    try:\n",
    "        prior_correct = eval(test['prior_group_answers_correct'].iloc[0])\n",
    "        prior_correct = [a for a in prior_correct if a != -1]\n",
    "    except:\n",
    "        prior_correct = []\n",
    "    \n",
    "    # Add prior correct to test and update stored users\n",
    "    if prior_correct:\n",
    "        prior_test.insert(s_train.answered_correctly_index, 'answered_correctly', prior_correct)\n",
    "        for uid, user in prior_test.groupby('user_id'):\n",
    "            s_train.update_user(\n",
    "                uid, user.drop(columns='user_id').to_numpy())\n",
    "\n",
    "    # Filter test\n",
    "    test = test.loc[\n",
    "        test['content_type_id'] == 0,\n",
    "        columns\n",
    "    ]\n",
    "\n",
    "    # Add global features\n",
    "    test, _, _ = transform_df(test, questions)\n",
    "\n",
    "    # Save test for later\n",
    "    prior_test = test.drop(columns='row_id').copy()\n",
    "\n",
    "    # Make x\n",
    "    x = np.apply_along_axis(\n",
    "        s_train.get_user_for_inference,\n",
    "        1,\n",
    "        test.drop(columns='row_id').to_numpy()\n",
    "    )\n",
    "\n",
    "    # Predict\n",
    "    test['answered_correctly'] = model.predict(x, batch_size=x.shape[0])\n",
    "    \n",
    "    env.predict(test[['row_id', 'answered_correctly']])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 1178.560018,
   "end_time": "2020-11-12T22:04:54.847036",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-11-12T21:45:16.287018",
   "version": "2.1.0"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}